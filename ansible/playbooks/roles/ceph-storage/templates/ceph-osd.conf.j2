# {{ ansible_managed }}

##################
## osd
# You need at least one.  Two or more if you want data to be replicated.
# Define as many as you like.
[osd]
### http://ceph.com/docs/giant/rados/configuration/osd-config-ref/

    # The path to the OSDs data.
    # You must create the directory when deploying Ceph.
    # You should mount a drive for OSD data at this mount point.
    # We do not recommend changing the default.
    # Type: String
    # Default: /var/lib/ceph/osd/$cluster-$id
    ;osd data                     = /var/lib/ceph/osd/$name

    ## You can change the number of recovery operations to speed up recovery
    ## or slow it down if your machines can't handle it

    # The number of active recovery requests per OSD at one time.
    # More requests will accelerate recovery, but the requests
    # places an increased load on the cluster.
    # Type: 32-bit Integer
    # (Default: 5)
    ;osd recovery max active      = 3

    # The maximum number of backfills allowed to or from a single OSD.
    # Type: 64-bit Integer
    # (Default: 10)
    ;osd max backfills            = 5

    # The maximum number of simultaneous scrub operations for a Ceph OSD Daemon.
    # Type: 32-bit Int
    # (Default: 1)
    ;osd max scrubs               = 2

    # You may add settings for ceph-deploy so that it will create and mount
    # the correct type of file system. Remove the comment `#` character for
    # the following settings and replace the values in parenthesis
    # with appropriate values, or leave the following settings commented
    # out to accept the default values.

    #osd mkfs type = {fs-type}
    #osd mkfs options {fs-type}   = {mkfs options}   # default for xfs is "-f"
    #osd mount options {fs-type}  = {mount options} # default mount option is "rw, noatime"
    ;osd mkfs type                = btrfs
    ;osd mount options btrfs      = noatime,nodiratime

    ## Ideally, make this a separate disk or partition.  A few
    ## hundred MB should be enough; more if you have fast or many
    ## disks.  You can use a file under the osd data dir if need be
    ## (e.g. /data/$name/journal), but it will be slower than a
    ## separate disk or partition.
    # The path to the OSD's journal. This may be a path to a file or a block
    # device (such as a partition of an SSD). If it is a file, you must
    # create the directory to contain it.
    # We recommend using a drive separate from the osd data drive.
    # Type: String
    # Default: /var/lib/ceph/osd/$cluster-$id/journal
    ;osd journal                  = /var/lib/ceph/osd/$name/journal

    # Check log files for corruption. Can be computationally expensive.
    # Type: Boolean
    # (Default: false)
    ;osd check for log corruption = true

### http://ceph.com/docs/giant/rados/configuration/journal-ref/

    # The size of the journal in megabytes. If this is 0,
    # and the journal is a block device, the entire block device is used.
    # Since v0.54, this is ignored if the journal is a block device,
    # and the entire block device is used.
    # Type: 32-bit Integer
    # (Default: 5120)
    # Recommended: Begin with 1GB. Should be at least twice the product
    # of the expected speed multiplied by "filestore max sync interval".
    ;osd journal size             = 2048     ; journal size, in megabytes

    ## If you want to run the journal on a tmpfs, disable DirectIO
    # Enables direct i/o to the journal.
    # Requires "journal block align" set to "true".
    # Type: Boolean
    # Required: Yes when using aio.
    # (Default: true)
    journal dio                   = {{ ceph_osd_journal_dio }}

    # osd logging to debug osd issues, in order of likelihood of being helpful
    ;debug ms                     = 1
    ;debug osd                    = 20
    ;debug filestore              = 20
    ;debug journal                = 20

### http://ceph.com/docs/giant/rados/configuration/filestore-config-ref/

    # The maximum interval in seconds for synchronizing the filestore.
    # Type: Double (optional)
    # (Default: 5)
    ;filestore max sync interval = 5

    # Enable snapshots for a btrfs filestore.
    # Type: Boolean
    # Required: No. Only used for btrfs.
    # (Default: true)
    ;filestore btrfs snap        = false

    # Defines the maximum number of in progress operations the file store
    # accepts before blocking on queuing new operations.
    # Type: Integer
    # Required: No. Minimal impact on performance.
    # (Default: 500)
    ;filestore queue max ops      = 500

    ## Filestore and OSD settings can be tweak to achieve better performance

### http://ceph.com/docs/giant/rados/configuration/filestore-config-ref/#misc

    # Min number of files in a subdir before merging into parent NOTE: A negative value means to disable subdir merging
    # Type: Integer
    # Required: No
    # Default:  10
    ;filestore merge threshold    = 10

    # filestore_split_multiple * abs(filestore_merge_threshold) * 16 is the maximum number of files in a subdirectory before splitting into child directories.
    # Type: Integer
    # Required: No
    # Default:  2
    ;filestore split multiple     = 2

    # The number of filesystem operation threads that execute in parallel.
    # Type: Integer
    # Required: No
    # Default:  2
    ;filestore op threads         = 4

    # The number of threads to service Ceph OSD Daemon operations. Set to 0 to disable it. Increasing the number may increase the request processing rate.
    # Type: 32-bit Integer
    # Default:  2
    ;osd op threads               = 2

    ## CRUSH

    # By default OSDs update their details (location, weight and root) on the CRUSH map during startup
    # Type: Boolean
    # Required: No;
    # (Default: true)
    ;osd crush update on start    = false


;[osd.0]
;    host                         = delta

;[osd.1]
;    host                         = epsilon

;[osd.2]
;    host                         = zeta

;[osd.3]
;    host                         = eta

